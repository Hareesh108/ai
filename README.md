# ğŸ§  AI â€“ Experiments & Practice

This repository is dedicated to my **AI/ML learning journey and practice**.  
Here, Iâ€™ll be pushing code, experiments, notes, and resources related to **Artificial Intelligence, Deep Learning, and Large Language Models (LLMs).**

---

## ğŸš€ Whatâ€™s Inside?

- ğŸ”¬ Practice code for AI/ML/Deep Learning  
- ğŸ“š Notes on Transformer architecture & modern NLP techniques  
- ğŸ§© Implementations of key AI concepts (Tokenization, Embeddings, Attention, etc.)  
- ğŸ—ï¸ Experiments with state-of-the-art LLMs  

---

## ğŸ”— Top 10 Popular LLMs

Here are some of the most important LLMs you can explore:

1. [GPT-4 (OpenAI)](https://openai.com/research/gpt-4)  
2. [Claude 3 (Anthropic)](https://www.anthropic.com/claude)  
3. [Gemini (Google DeepMind)](https://deepmind.google/technologies/gemini/)  
4. [LLaMA 3 (Meta AI)](https://ai.meta.com/llama/)  
5. [Mistral (Mistral AI)](https://mistral.ai/news/announcing-mistral-7b/)  
6. [Falcon (TII)](https://tii.ae/falcon)  
7. [Cohere Command R](https://cohere.com/command)  
8. [Jurassic-2 (AI21 Labs)](https://www.ai21.com/studio)  
9. [Mixtral (Mistral AI, MoE)](https://mistral.ai/news/mixtral-of-experts/)  
10. [DeepSeek LLM](https://deepseek.com/)  

---

## ğŸ”¥ Current Trends in LLMs

The field of LLMs is rapidly evolving. Some of the **latest research and engineering trends** include:

1. **Reasoning (RLVR, GRPO)**  
   - *RLVR (Reinforcement Learning via Verifiable Rewards):* ensures models get rewarded for provably correct reasoning steps.  
   - *GRPO (Generalized Reinforcement Policy Optimization):* an advanced RL method for improving structured reasoning.  

2. **MoEs (Mixture of Experts)**  
   - Models with multiple expert subnetworks, where only a subset activates per query.  
   - Enables efficiency and scalability.  
   - Example: *Mixtral of Experts*.  

3. **Tool Use**  
   - LLMs can call external tools (APIs, calculators, databases, code execution).  
   - Moves LLMs from static text generators to **active AI agents**.  

4. **Multi-Head Latent Attention**  
   - Extends multi-head attention to focus on latent (hidden) representations.  
   - Enhances reasoning depth and compression of long contexts.  

## ğŸ“š References

- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)  
- [The Illustrated Transformer â€“ Jay Alammar](http://jalammar.github.io/illustrated-transformer/)  

---

## ğŸ› ï¸ Author

**Hareesh Bhittam**  
Full-stack developer exploring **AI & LLMs**.
